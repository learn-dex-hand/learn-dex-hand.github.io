<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
  <title>Learning Dexterous Manipulation Workshop, RSS 2023</title>
  <meta name="description" content="Website for the Learning Dexterous Manipulation workshop at the RSS 2023">
  <meta name="author" content='Nur Muhammad \"Mahi\" Shafiullah'>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <!--[if lte IE 8]>
  <script src="assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="assets/css/main.css"/>
  <!--[if lte IE 9]>
  <link rel="stylesheet" href="assets/css/ie9.css"/><![endif]-->
  <!--[if lte IE 8]>
  <link rel="stylesheet" href="assets/css/ie8.css"/><![endif]-->
  <link rel="stylesheet" href="assets/css/lightbox.css"/>
</head>

<body>

<!-- Wrapper -->
<div id="wrapper">

  <!-- Header -->
  <header id="header" class="alt">
    <!-- <span class="logo"><img src="images/bg_new.jpeg" alt="" /></span>  -->

    <h1><b>Learning Dexterous Manipulation</b></h1>
    <h2>Workshop at the Robotics: Science and Systems - RSS 2023<br>
      Daegu, Korea, July 14 2023, half-day workshop</h2>
  </header>

  <!-- Nav -->
  <nav id="nav">
    <ul>
      <li><a href="#about" class="active">About</a></li>
      <li><a href="#speakers">Speakers</a></li>
      <li><a href="#cfp">Call for papers</a></li>
      <li><a href="#schedule">Schedule</a></li>
      <li><a href="#organizers">Organizers</a></li>
    </ul>
  </nav>

  <!-- Main -->
  <div id="main">

    <section id="about" class="main">
      <div class="spotlight">
        <div class="content">
          <header class="major">
            <h2>About</h2>
          </header>

          <p>
            The workshop titled "Learning Dexterous Manipulation" aims to investigate learning-based
            approaches for dexterous manipulation with a high level of generalizability. Dexterous
            manipulation has been one of the most challenging problems in robotics, and this workshop
            intends to offer insights and perspectives to researchers and participants on this topic.
            Additionally, the latest advancements in various sensing technologies will also be
            discussed. The ultimate goal of the workshop is to equip participants with the knowledge and
            skills necessary to design and develop advanced robotic systems capable of performing
            complex manipulation tasks with perception and enhancing human-robot interaction and
            collaboration.
          </p>
          <p>
            This workshop is intended for researchers, engineers, and students who have a
            solid background in learning-based approaches, computer vision, or other related fields, and
            are interested in robotics and robot sensing. The presenters and panelists for the workshop
            will include experts from both academic and industrial backgrounds, representing a variety
            of disciplines, such as robot learning, robotics, mechanical engineering, and robot sensing.
            Accepted papers will have a chance to be presented during the poster session, and selected
            papers will be featured in contributed talks. The workshop will be promoted through relevant
            mailing lists of universities and research institutes, as well as social media platforms.

            Here are the topics we are interested in, covering recent
            advancements and open questions in the context of learning dexterous manipulation.

          <ul>
            <li><strong>Data for Dexterous Manipulation:</strong></li>
            <ul>
              <li>Can human hand data for dexterous manipulation be collected in a general way using
                any
                expert-grade equipment? What is the data gap between human and robot hands?
              </li>
              <li>How can we improve current data collection methods, such as teleoperation, to
                facilitate
                large-scale data collection?
              </li>
            </ul>
            <li><strong>Computer Vision:</strong></li>
            <ul>
              <li>How can occlusion between objects and robot hands during dexterous manipulation be
                addressed?
              </li>
              <li>How can policies generalize to the open world outside the lab environment,
                considering
                the relatively unpredictable changes in outdoor lighting and the vast amount of
                information that needs to be processed?
              </li>
            </ul>
            <li><strong>Tactile Information:</strong></li>
            <ul>
              <li>How can tactile information help robots better accomplish tasks and perceive their
                environment?
              </li>
              <li>What kind of tactile information is best suited for dexterous robot hands, and can
                it
                compensate for the shortcomings of visual perception?
              </li>
            </ul>
            <li><strong>Robot learning</strong></li>
            <ul>
              <li>Will we see a unified and generalized model for most daily dexterous manipulation
                tasks
                or a specialized model for each individual task?
              </li>
              <li>How can learning-based policies handle dynamic tasks that require high-frequency
                control
                and detailed dynamics models?
              </li>
            </ul>
          </ul>

          We hope to connect researchers from the communities of dexterous robotics, representation
          learning, computer vision, and to induce collaborations in this exciting new domain, while
          providing a platform to discuss recent developments, challenges and tradeoffs.

          </p>

        </div>
      </div>
    </section>

    <section id="speakers" class="main">
      <div class="spotlight">
        <div class="content">
          <header class="major">
            <h2>Speakers and panelists</h2>
          </header>
          <!-- <h3>Invited talks</h3> -->
          <ul class="features">
            <li>
              <img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
                   src="https://people.csail.mit.edu/pulkitag/images/pulkit.jpg" alt=""/>
              <h3><a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agarwal</a></br>
                MIT CSAIL</h3>
            </li>
            <li>
              <img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
                   src="https://web.stanford.edu/~bohg/img/portrait_square.png" alt=""/>
              <h3><a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a></br>Stanford
                University</h3>
            </li>
            <li>
              <img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
                   src="https://www.me.columbia.edu/files/seas/styles/1700x836/public/content/bio_banner_image/2017/19/ciocarlie_eileen_b.jpg"
                   alt=""/>
              <h3><a href="https://www.me.columbia.edu/faculty/matei-ciocarlie">Matei
                Ciocarlie</a></br>Columbia University</h3>
            </li>
          </ul>
          <ul class="features">
            <li>
              <img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
                   src="https://homes.cs.washington.edu/~abhgupta/images/abhgupta_sm.jpeg" alt=""/>
              <h3><a href="https://homes.cs.washington.edu/~abhgupta/">Abhishek
                Gupta</a></br>University of Washington</h3>
            </li>
            <li>
              <img style=" width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
                   src="https://vikashplus.github.io/images/image.png" alt=""/>
              <h3><a href="https://vikashplus.github.io/AboutMe.html" target="_blank">Vikash
                Kumar</a></br>Meta AI Research</h3>
            </li>
          </ul>
        </div>
      </div>
    </section>

    <!-- make a section with the schedule for the workshop -->
    <section id="schedule" class="main">
      <div class="spotlight">
        <div class="content">
          <header class="major">
            <h2>Schedule</h2>
          </header>

          <h3>July 14th 2023</h3>
          <p>Times are given in Korea Standard Time (UTC+09:00)</p>
          <ul>
            <li>1:30pm - 1:45pm: Welcome and Online login</li>
            <li>1:45pm - 2:15pm: Invited Talk 1: Pulkit Agarwal</li>
            <li>2:15pm - 2:45pm: Invited Talk 2: Jeannette Bohg</li>
            <li>2:45pm - 3:30pm: Coffee break and poster session</li>
            <li>3:30pm - 4:00pm: Invited Talk 3: Matei Ciocarlie</li>
            <li>4:00pm - 4:30pm: Invited Talk 4: Abhishek Gupta</li>
            <li>4:30pm - 5:00pm: Contributed Spotlight Talks</li>
            <li>5:00pm - 5:30pm: Invited Talk 5: Vikash Kumar</li>
            <li>5:30pm - 5:45pm: Closing remarks</li>
          </ul>
        </div>
      </div>
    </section>


    <section id="cfp" class="main">
      <div class="spotlight">
        <div class="content">
          <header class="major">
            <h2>Call for papers</h2>
          </header>

          <h3>Important dates (all times AoE)</h3>
          <ul>
            <li>Submissions open: April 19 2023</li>
            <li>Submission deadline: <b>June 2 2023 (11:59pm Anywhere on Earth)</b></li>
            <li>Decision notification: June 16 2023</li>
            <li>Camera ready deadline: July 7 2023</li>
            <li>Workshop: July 14th 2023</li>
          </ul>

          <h3>Call for papers</h3>

          <p> Submission link: <a href="https://cmt3.research.microsoft.com/RSSLDM2023">https://cmt3.research.microsoft.com/RSSLDM2023</a>
          </p>

          <p>In this workshop, we aim to bring together machine learning and robotics researchers who work
            at the intersection of these fields.
            We invite researchers to submit work in the following or related areas (non-exhaustive
            list):</p>
          <ul>
            <li><strong>Data for Dexterous Manipulation:</strong></li>
            <ul>
              <li>Can human hand data for dexterous manipulation be collected in a general way using
                any
                expert-grade equipment? What is the data gap between human and robot hands?
              </li>
              <li>How can we improve current data collection methods, such as teleoperation, to
                facilitate
                large-scale data collection?
              </li>
            </ul>
            <li><strong>Computer Vision:</strong></li>
            <ul>
              <li>How can occlusion between objects and robot hands during dexterous manipulation be
                addressed?
              </li>
              <li>How can policies generalize to the open world outside the lab environment,
                considering
                the relatively unpredictable changes in outdoor lighting and the vast amount of
                information that needs to be processed?
              </li>
            </ul>
            <li><strong>Tactile Information:</strong></li>
            <ul>
              <li>How can tactile information help robots better accomplish tasks and perceive their
                environment?
              </li>
              <li>What kind of tactile information is best suited for dexterous robot hands, and can
                it
                compensate for the shortcomings of visual perception?
              </li>
            </ul>
            <li><strong>Robot learning</strong></li>
            <ul>
              <li>Will we see a unified and generalized model for most daily dexterous manipulation
                tasks
                or a specialized model for each individual task?
              </li>
              <li>How can learning-based policies handle dynamic tasks that require high-frequency
                control
                and detailed dynamics models?
              </li>
            </ul>
            <li>Any other related topics we might have forgotten in the list above &#128516;</li>
          </ul>


          <h4>Accepted Talks and Posters</h4>
          <p>Accepted papers will be presented in the form of posters (with lightning talks) or spotlight
            talks at the workshop. We encourage submissions of work in progress, as well as work that is
            not yet published. </p>

          <h3>Submission instructions</h3>

          <ul>
            <li>Kindly utilize the <a href="https://roboticsconference.org/docs/paper-template-latex.tar.gz">RSS 2023
              template</a> when submitting your paper. Including supplementary material is optional and only required if
              you wish to offer additional details or video demonstrations.
            </li>
            <li>Submissions should be short papers up to <b>4 pages in PDF format</b> (not counting references
              and an optional appendix, which can go over the limit)
            </li>
            <li>All submitted materials will undergo a double-blind review process. While this workshop will not produce
              formal official proceedings, the accepted papers will be accessible on the workshop website. As this does
              not count as an archival publication. Consequently,
              authors are at liberty to publish their work in archival journals or conferences.
            </li>
          </ul>
        </div>
      </div>
    </section>


    <section id="organizers" class="main">
      <div class="spotlight">
        <div class="content">
          <header class="major">
            <h2>Organizers</h2>
          </header>

          <ul class="features">
            <li>
              <img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
                   src="https://contactrika.github.io/img/rika_photo_one.jpg" alt=""/>
              <h3><a href="https://contactrika.github.io/">Rika Antonova</a></br>Postdoctoral Scholar
                at
                Stanford</h3>
            </li>
            <li>
              <img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
                   src="images/ah.jpg" alt=""/>
              <h3><a href="https://ankurhanda.github.io/">Ankur Handa</a></br>Research Scientist at
                NVIDIA
              </h3>
            </li>
            <li>
              <img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
                   src="images/bh.jpg" alt=""/>
              <h3><a href="https://binghao-huang.github.io/">Binghao Huang</a></br>Ph.D. student at
                UCSD
              </h3>
            </li>
            <li>
              <img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
                   src="https://www.lerrelpinto.com/authors/admin/avatar_hu4fd532323075888c71e9261145e3a1d5_40812_200x200_fill_q75_lanczos_center.jpg"
                   alt=""/>
              <h3><a href="https://lerrelpinto.com/">Lerrel
                Pinto</a></br>Assistant Professor at NYU</h3>
            </li>
            <!-- </ul>
          <ul class="features"> -->
            <li>
              <img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
                   src="https://yzqin.github.io/file/qyz_circle.png" alt=""/>
              <h3><a href="https://yzqin.github.io/">Yuzhe Qin</a></br>Ph.D. student at UCSD</h3>
            </li>
            <li>
              <img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
                   src="https://mahis.life/assets/images/self/mahi_profile_comp.jpeg" alt=""/>
              <h3><a href="https://mahis.life/">Mahi Shafiullah</a></br>Ph.D. student at NYU
              </h3>
            </li>
            <li>
              <img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
                   src="https://xiaolonw.github.io/static/profile.jpg" alt=""/>
              <h3><a href="https://xiaolonw.github.io/">Xiaolong Wang</a></br>Assistant Professor
                at UCSD</h3>
            </li>
            <li>
              <img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
                   src="https://ericyi.github.io/Li_Yi_files/ericyi.jpg" alt=""/>
              <h3><a href="https://ericyi.github.io/">Li Yi</a></br>Assistant Professor at THU
              </h3>
            </li>
            <li>
              <img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
                   src="http://robotouch.ri.cmu.edu/yuanwz/index_files/image002.jpg" alt=""/>
              <h3><a href="http://robotouch.ri.cmu.edu/yuanwz/">Wenzhen Yuan</a></br>Assistant
                Professor
                at CMU</h3>
            </li>
          </ul>
        </div>
      </div>
    </section>

    <section id="contact" class="main">
      <div class="spotlight">
        <div class="content">
          <header class="major">
            <h2>Contact</h2>
          </header>
          <p>For questions and comments, please <a
              href="mailto:learning-dexterous-manipulation@googlegroups.com">contact us</a>.
          </p>
        </div>
      </div>
    </section>

  </div>

  <!-- Footer -->
  <footer id="footer">
    <p class="copyright">Copyright &copy; Mahi Shafiullah. Design: <a href="https://html5up.net">HTML5
      UP</a>.</br>Design inspired by <a
        href="http://bayesiandeeplearning.org/">http://bayesiandeeplearning.org/</a> by Yarin Gal and <a
        href="https://microsoft.github.io/robotics.pretraining.workshop.icra/">Pretraining for Robotics</a>
      by Sai Vemprala.</p>
  </footer>

</div>

<!-- Scripts  -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.scrollex.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/skel.min.js"></script>
<script src="assets/js/util.js"></script>
<!--[if lte IE 8]>
<script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="assets/js/main.js"></script>

<!-- End of Statcounter Code -->
<script src="assets/js/lightbox.js"></script>
</body>

</html>