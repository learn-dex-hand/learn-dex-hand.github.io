<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Learning Dexterous Manipulation Workshop, RSS 2023</title>
	<meta name="description" content="Website for the Learning Dexterous Manipulation workshop at the RSS 2023">
	<meta name="author" content='Nur Muhammad \"Mahi\" Shafiullah'>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	<link rel="stylesheet" href="assets/css/lightbox.css" />
</head>

<body>

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Header -->
		<header id="header" class="alt">
			<!-- <span class="logo"><img src="images/bg_new.jpeg" alt="" /></span>  -->

			<h1><b>Learning Dexterous Manipulation</b></h1>
			<h2>Workshop at the Robotics: Science and Systems - RSS 2023<br>
				Daegu, Korea, July 14 2023, half-day workshop</h2>
		</header>

		<!-- Nav -->
		<nav id="nav">
			<ul>
				<li><a href="#about" class="active">About</a></li>
				<li><a href="#speakers">Speakers</a></li>
				<li><a href="#cfp">Call for papers</a></li>
				<li><a href="#schedule">Schedule</a></li>
				<li><a href="#organizers">Organizers</a></li>
			</ul>
		</nav>

		<!-- Main -->
		<div id="main">

			<section id="about" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>About</h2>
						</header>

						<p>
							The workshop titled "Learning Dexterous Manipulation" aims to investigate learning-based
							approaches for dexterous manipulation with a high level of generalizability. Dexterous
							manipulation has been one of the most challenging problems in robotics, and this workshop
							intends to offer insights and perspectives to researchers and participants on this topic.
							Additionally, the latest advancements in various sensing technologies will also be
							discussed. The ultimate goal of the workshop is to equip participants with the knowledge and
							skills necessary to design and develop advanced robotic systems capable of performing
							complex manipulation tasks with perception and enhancing human-robot interaction and
							collaboration.
						</p>
						<p>
							This workshop is intended for researchers, engineers, and students who have a
							solid background in learning-based approaches, computer vision, or other related fields, and
							are interested in robotics and robot sensing. The presenters and panelists for the workshop
							will include experts from both academic and industrial backgrounds, representing a variety
							of disciplines, such as robot learning, robotics, mechanical engineering, and robot sensing.
							Accepted papers will have a chance to be presented during the poster session, and selected
							papers will be featured in contributed talks. The workshop will be promoted through relevant
							mailing lists of universities and research institutes, as well as social media platforms.

							Here are the topics we are interested in, covering recent
							advancements and open questions in the context of learning dexterous manipulation.

						<ul>
							<li><strong>Data for Dexterous Manipulation:</strong></li>
							<ul>
								<li>Can human hand data for dexterous manipulation be collected in a general way using
									any
									expert-grade equipment? What is the data gap between human and robot hands?</li>
								<li>How can we improve current data collection methods, such as teleoperation, to
									facilitate
									large-scale data collection?</li>
							</ul>
							<li><strong>Computer Vision:</strong></li>
							<ul>
								<li>How can occlusion between objects and robot hands during dexterous manipulation be
									addressed?</li>
								<li>How can policies generalize to the open world outside the lab environment,
									considering
									the relatively unpredictable changes in outdoor lighting and the vast amount of
									information that needs to be processed?</li>
							</ul>
							<li><strong>Tactile Information:</strong></li>
							<ul>
								<li>How can tactile information help robots better accomplish tasks and perceive their
									environment?</li>
								<li>What kind of tactile information is best suited for dexterous robot hands, and can
									it
									compensate for the shortcomings of visual perception?</li>
							</ul>
							<li><strong>Robot learning</strong></li>
							<ul>
								<li>Will we see a unified and generalized model for most daily dexterous manipulation
									tasks
									or a specialized model for each individual task?</li>
								<li>How can learning-based policies handle dynamic tasks that require high-frequency
									control
									and detailed dynamics models?</li>
							</ul>
						</ul>

						We hope to connect researchers from the communities of dexterous robotics, representation
						learning, computer vision, and to induce collaborations in this exciting new domain, while
						providing a platform to discuss recent developments, challenges and tradeoffs.

						</p>

					</div>
				</div>
			</section>

			<section id="speakers" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Speakers and panelists</h2>
						</header>
						<!-- <h3>Invited talks</h3> -->
						<ul class="features">
							<li>
								<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://people.csail.mit.edu/pulkitag/images/pulkit.jpg" alt="" />
								<h3><a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agarwal</a></br>
									MIT CSAIL</h3>
							</li>
							<li>
								<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://web.stanford.edu/~bohg/img/portrait_square.png" alt="" />
								<h3><a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a></br>Stanford
									University</h3>
							</li>
							<li>
								<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://www.me.columbia.edu/files/seas/styles/1700x836/public/content/bio_banner_image/2017/19/ciocarlie_eileen_b.jpg"
									alt="" />
								<h3><a href="https://www.me.columbia.edu/faculty/matei-ciocarlie">Matei
										Ciocarlie</a></br>Columbia University</h3>
							</li>
						</ul>
						<ul class="features">
							<li>
								<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://homes.cs.washington.edu/~abhgupta/images/abhgupta_sm.jpeg" alt="" />
								<h3><a href="https://homes.cs.washington.edu/~abhgupta/">Abhishek
										Gupta</a></br>University of Washington</h3>
							</li>
							<li>
								<img style=" width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://vikashplus.github.io/images/image.png" alt="" />
								<h3><a href="https://vikashplus.github.io/AboutMe.html" target="_blank">Vikash
										Kumar</a></br>Meta AI Research</h3>
							</li>
						</ul>
					</div>
				</div>
			</section>

			<!-- make a section with the schedule for the workshop -->
			<section id="schedule" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Schedule</h2>
						</header>

						<h3>July 14th 2023</h3>
						<ul>
							<li>8:30am - 8:50am: Breakfast</li>
							<li>8:50am - 9:00am: Introduction and opening remarks</li>
							<li>9:00am - 9:30am: Invited Talk 1</li>
							<li>9:30am - 10:00am: Invited Talk 2</li>
							<li>10:00am - 10:15am: Poster lightning talks</li>
							<li>10:15am - 11:00am: Coffee break and poster session I</li>
							<li>11:00am - 11:30pm: Invited Talk 3</li>
							<li>11:30am - 12:00pm: Invited Talk 4</li>
							<li>12:00pm - 1:00pm: Lunch break</li>
							<li>1:00pm - 1:30pm: Invited Talk 5</li>
							<li>1:30pm - 2:00pm: Invited Talk 6</li>
							<li>2:00pm - 2:30pm: Spotlight talks</li>
							<li>2:30pm - 3:15pm: Coffee break and poster session II</li>
							<li>3:15pm - 3:45pm: Invited Talk 7</li>
							<li>3:45pm - 4:15pm: Invited Talk 8</li>
							<li>4:15pm - 5:00pm: Panel discussion</li>
							<li>5:00pm - 5:05pm: Closing remarks</li>
						</ul>
					</div>
				</div>
			</section>


			<section id="cfp" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Call for papers</h2>
						</header>

						<h3>Important dates (all times AoE)</h3>
						<ul>
							<li>Submissions open: TBD 2023</li>
							<li>Submission deadline: TBD 2023</li>
							<li>Decision notification: TBD 2023</li>
							<li>Camera ready deadline: TBD 2023</li>
							<li>Workshop: July 14th 2023</li>
						</ul>

						<h3>Call for papers</h3>

						<p> Submission link: <a href="https://openreview.net/">TBD</a>
						</p>

						<p>In this workshop, we aim to bring together machine learning and robotics researchers who work
							at the intersection of these fields.
							We invite researchers to submit work in the following or related areas (non-exhaustive
							list):</p>
						<ul>
							<li><strong>Data for Dexterous Manipulation:</strong></li>
							<ul>
								<li>Can human hand data for dexterous manipulation be collected in a general way using
									any
									expert-grade equipment? What is the data gap between human and robot hands?</li>
								<li>How can we improve current data collection methods, such as teleoperation, to
									facilitate
									large-scale data collection?</li>
							</ul>
							<li><strong>Computer Vision:</strong></li>
							<ul>
								<li>How can occlusion between objects and robot hands during dexterous manipulation be
									addressed?</li>
								<li>How can policies generalize to the open world outside the lab environment,
									considering
									the relatively unpredictable changes in outdoor lighting and the vast amount of
									information that needs to be processed?</li>
							</ul>
							<li><strong>Tactile Information:</strong></li>
							<ul>
								<li>How can tactile information help robots better accomplish tasks and perceive their
									environment?</li>
								<li>What kind of tactile information is best suited for dexterous robot hands, and can
									it
									compensate for the shortcomings of visual perception?</li>
							</ul>
							<li><strong>Robot learning</strong></li>
							<ul>
								<li>Will we see a unified and generalized model for most daily dexterous manipulation
									tasks
									or a specialized model for each individual task?</li>
								<li>How can learning-based policies handle dynamic tasks that require high-frequency
									control
									and detailed dynamics models?</li>
							</ul>
							<li>Any other related topics we might have forgotten in the list above &#128516;</li>
						</ul>


						<h4>Accepted Talks and Posters</h4>
						<p>Accepted papers will be presented in the form of posters (with lightning talks) or spotlight
							talks at the workshop. We encourage submissions of work in progress, as well as work that is
							not yet published. </p>

						<h3>Submission instructions</h3>

						<ul>
							<li>Submissions should be short papers up to 4 pages in PDF format (not counting references
								and an optional appendix, which can go over the limit) </li>
							<li>This workshop will not provide formal official proceedings and the papers will be
								available on the workshop website.</li>
						</ul>

					</div>
				</div>
			</section>


			<section id="organizers" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Organizers</h2>
						</header>

						<ul class="features">
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/rb.jpeg" alt="" />
								<h3><a href="https://contactrika.github.io/">Rika Antonova</a></br>Postdoctoral Scholar
									at
									Stanford</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/sv.png" alt="" />
								<h3><a href="https://ankurhanda.github.io/">Ankur Handa</a></br>Research Scientist at
									NVIDIA
								</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/mm.jpeg	" alt="" />
								<h3><a href="https://binghao-huang.github.io/">Binghao Huang</a></br>Ph.D. student at
									UCSD
								</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/lf.jpeg" alt="" />
								<h3><a href="https://lerrelpinto.com/">Lerrel
										Pinto</a></br>Assistant Professor at NYU</h3>
							</li>
							<!-- </ul>
						<ul class="features"> -->
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/al.jpeg" alt="" />
								<h3><a href="https://yzqin.github.io/">Yuzhe Qin</a></br>Ph.D. student at UCSD</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/xl.png" alt="" />
								<h3><a href="https://mahis.life/">Mahi Shafiullah</a></br>Ph.D. student at NYU
								</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/vb.jpeg" alt="" />
								<h3><a href="https://xiaolonw.github.io/">Xiaolong Wang</a></br>Assistant Professor
									at UCSD</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/vb.jpeg" alt="" />
								<h3><a href="https://ericyi.github.io/">Li Yi</a></br>Assistant Professor at THU
								</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/vb.jpeg" alt="" />
								<h3><a href="http://robotouch.ri.cmu.edu/yuanwz/">Wenzhen Yuan</a></br>Assistant
									Professor
									at CMU</h3>
							</li>
						</ul>
					</div>
				</div>
			</section>

			<section id="contact" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Contact</h2>
						</header>
						<p>For questions and comments, please <a
								href="mailto:learning-dexterous-manipulation@googlegroups.com">contact us</a>.
						</p>
					</div>
				</div>
			</section>

		</div>

		<!-- Footer -->
		<footer id="footer">
			<p class="copyright">Copyright &copy; Mahi Shafiullah. Design: <a href="https://html5up.net">HTML5
					UP</a>.</br>Design inspired by <a
					href="http://bayesiandeeplearning.org/">http://bayesiandeeplearning.org/</a> by Yarin Gal and <a
					href="https://microsoft.github.io/robotics.pretraining.workshop.icra/">Pretraining for Robotics</a>
				by Sai Vemprala.</p>
		</footer>

	</div>

	<!-- Scripts  -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/skel.min.js"></script>
	<script src="assets/js/util.js"></script>
	<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="assets/js/main.js"></script>

	<!-- End of Statcounter Code -->
	<script src="assets/js/lightbox.js"></script>
</body>

</html>