<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Learning Dexterous Manipulation Workshop, RSS 2023</title>
	<meta name="description" content="Website for the Learning Dexterous Manipulation workshop at the RSS 2023">
	<meta name="author" content='Nur Muhammad \"Mahi\" Shafiullah'>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<!--[if lte IE 8]>
  <script src="assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="assets/css/main.css" />
	<!--[if lte IE 9]>
  <link rel="stylesheet" href="assets/css/ie9.css"/><![endif]-->
	<!--[if lte IE 8]>
  <link rel="stylesheet" href="assets/css/ie8.css"/><![endif]-->
	<link rel="stylesheet" href="assets/css/lightbox.css" />
</head>

<body>

	<div id="wrapper">

		<!-- Header -->
		<header id="header" class="alt">
			<!-- <span class="logo"><img src="images/bg_new.jpeg" alt="" /></span>  -->

			<h1><b>Learning Dexterous Manipulation</b></h1>
			<h2>Workshop at the Robotics: Science and Systems - RSS 2023<br>
				Daegu, Korea, July 14 2023, half-day workshop</h2>
		</header>

		<!-- Nav -->
		<nav id="nav">
			<ul>
				<li><a href="#about" class="active">About</a></li>
				<li><a href="#speakers">Speakers</a></li>
				<li><a href="#cfp">Call for papers</a></li>
				<li><a href="#schedule">Schedule</a></li>
				<li><a href="#papers">Papers</a></li>
				<li><a href="#organizers">Organizers</a></li>
			</ul>
		</nav>

		<!-- Main -->
		<div id="main">

			<section id="about" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>About</h2>
						</header>
						<p style="color: #cc8899;">
							<b>New:</b> Recording is now avaible on <a href="https://www.youtube.com/watch?v=PKsRnKh6Q24">YouTube</a>.
						</p>

						<p>
							The workshop titled "Learning Dexterous Manipulation" aims to investigate learning-based
							approaches for dexterous manipulation with a high level of generalizability. Dexterous
							manipulation has been one of the most challenging problems in robotics, and this
							workshop
							intends to offer insights and perspectives to researchers and participants on this
							topic.
							Additionally, the latest advancements in various sensing technologies will also be
							discussed. The ultimate goal of the workshop is to equip participants with the knowledge
							and
							skills necessary to design and develop advanced robotic systems capable of performing
							complex manipulation tasks with perception and enhancing human-robot interaction and
							collaboration.
						</p>
						<p>
							This workshop is intended for researchers, engineers, and students who have a
							solid background in learning-based approaches, computer vision, or other related fields,
							and
							are interested in robotics and robot sensing. The presenters and panelists for the
							workshop
							will include experts from both academic and industrial backgrounds, representing a
							variety
							of disciplines, such as robot learning, robotics, mechanical engineering, and robot
							sensing.
							Accepted papers will have a chance to be presented during the poster session, and
							selected
							papers will be featured in contributed talks. The workshop will be promoted through
							relevant
							mailing lists of universities and research institutes, as well as social media
							platforms.

							Here are the topics we are interested in, covering recent
							advancements and open questions in the context of learning dexterous manipulation.

							We hope to connect researchers from the communities of dexterous robotics, representation
							learning, computer vision, and to induce collaborations in this exciting new domain, while
							providing a platform to discuss recent developments, challenges and tradeoffs.

						</p>

					</div>
				</div>
			</section>

			<section id="speakers" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Speakers and panelists</h2>
						</header>
						<!-- <h3>Invited talks</h3> -->
						<ul class="features">
							<li>
								<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://people.csail.mit.edu/pulkitag/images/pulkit.jpg" alt="" />
								<h3><a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a></br>
									MIT CSAIL</h3>
							</li>
							<li>
								<img style=" width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://vikashplus.github.io/images/image.png" alt="" />
								<h3><a href="https://vikashplus.github.io/AboutMe.html" target="_blank">Vikash
										Kumar</a></br>Carnegie Melon University</h3>
							</li>
							<li>
								<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://tesshellebrekers.com/wp-content/uploads/2020/10/tess_edit-scaled.jpg"
									alt="" />
								<h3><a href="https://tesshellebrekers.com/">Tess Hellebrekers</a></br>Meta AI
									Research
								</h3>
							</li>
							<li>
								<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://homes.cs.washington.edu/~abhgupta/images/abhgupta_sm.jpeg" alt="" />
								<h3><a href="https://homes.cs.washington.edu/~abhgupta/">Abhishek
										Gupta</a></br>University of Washington</h3>
							</li>
							<li>
								<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://sferrazza.cc/img/headshot.jpg" alt="" />
								<h3><a href="https://sferrazza.cc/">Carmelo Sferrazza</a></br>UC Berkeley</h3>
							</li>
							<li>
								<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"
									src="https://fang-haoshu.github.io/images/me-2023.jpg" alt="" />
								<h3><a href="https://fang-haoshu.github.io/">Hao-Shu Fang</a></br>Shanghai Jiao Tong University</h3>
							</li>					
						</ul>
					</div>
				</div>
			</section>

			<!-- make a section with the schedule for the workshop -->
			<section id="schedule" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Schedule</h2>
						</header>

						<h3>July 14th 2023</h3>
						<p>Times are given in Korea Standard Time (UTC+09:00)</p>
						<ul>
							<li>1:30pm - 1:45pm: Welcome and Online login</li>
							<li>1:45pm - 2:15pm: Invited Talk 1: Pulkit Agrawal (remote)</li>
							<li>2:15pm - 2:45pm: Invited Talk 2: Vikash Kumar (remote)</li>
							<li>2:45pm - 3:15pm: Spotlight presentation</li>
							<li>3:15pm - 3:45pm: Coffee break and poster session</li>
							<li>3:45pm - 4:15pm: Invited Talk 3: Tess Hellebrekers</li>
							<li>4:15pm - 4:45pm: Invited Talk 4: Abhishek Gupta</li>
							<li>4:45pm - 5:15pm: Invited Student Speakers</li>
							<li>5:15pm - 5:30pm: Closing remarks</li>
						</ul>
					</div>
				</div>
			</section>

			<section id="papers" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Papers</h2>
						</header>
						<p>
							Learning a Universal Human Prior for Dexterous Manipulation from Human Preference <a
								href="https://drive.google.com/file/d/1tc459TDSjo_yZVzECIRqtWyRViUF5USF/view?usp=sharing">[PDF]</a>
						</p>
						<p>UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and
							Iterative Generalist-Specialist Learning <a
								href="https://drive.google.com/file/d/1C3AnxjOO1o0nqX3H6ObUIKoXlKwE_r6q/view?usp=sharing">[PDF]</a><a
								href="https://drive.google.com/file/d/1SXHqKRVVp75SxsWvc8ZRPXyiNrDE8lHv/view?usp=sharing">[Material]</a>
						</p>
						<p>Online augmentation of learned grasp sequence policies for more adaptable in-hand
							manipulation <a
								href="https://drive.google.com/file/d/1mWBGmp18V4USoVtBana_UBVuIUZ6SIYI/view?usp=sharing">[PDF]</a>
						</p>
						<p>Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations <a href="https://drive.google.com/file/d/1w_arPXsyDAe7DG_xIeW-i-bDKMIdOurK/view?usp=sharing">[PDF]</a>
						</p>

						<p>DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation <a href="https://drive.google.com/file/d/1d8s-3ev6wChan7Vx66Vetd1xMMucexc8/view?usp=sharing">[PDF]</a>
						</p>

						<p>Rotating without Seeing: Towards In-hand Dexterity through Touch <a href="https://drive.google.com/file/d/1pyiNRv7tZ35AG-hrYea7gZw-nqYicvAo/view?usp=sharing">[PDF]</a>
						</p>
						<p>Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play <a href="https://drive.google.com/file/d/1Ksn9yEHOCrv02UaaIezERA9FDx3i_F87/view?usp=sharing">[PDF]</a>
						</p>
						<p>DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects <a href="https://drive.google.com/file/d/1_nC5M-ma6kbaFKOMQkJKOT8AfjukfAh6/view?usp=sharing">[PDF]</a>
						</p>
						<p>A Robust and Accurate System for Data Acquisition of Dexterous Manipulation <a href="https://drive.google.com/file/d/1w2YKsl_xbttiPBsGFzlaR9dLmqoYP-K_/view?usp=sharing">[PDF]</a>
						</p>
						<p>Tactile Pose Feedback for Closed-loop Manipulation Tasks <a href="https://drive.google.com/file/d/1QZVAAoXrOp8DmSG3qzMIwxgyas6UTWib/view?usp=sharing">[PDF]</a>
						</p>
						<p>LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning <a href="https://drive.google.com/file/d/1QzgfOjeoI9JBQv5c_yRDw8nWULk7Fk9q/view?usp=sharing">[PDF]</a>
						</p>
						<p>DEFT: Dexterous Fine-Tuning for Real World, General Purpose Manipulation <a href="https://drive.google.com/file/d/1UUnqYM_HBPCEPV2KhEcxYYTeXZu3Fwt6/view?usp=sharing">[PDF]</a>
						</p>
						<p>On the Utility of Koopman Operator Theory in Learning Dexterous Manipulation Skills <a href="https://drive.google.com/file/d/1Ws1Ui5r4Yksk_jefYgtIS4N92WoUWeq0/view?usp=sharing">[PDF]</a>
						</p>
						<p>The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning <a href="https://drive.google.com/file/d/1iZNaSBbE_R6QuOxnL-_0OL6P5_UKiaxL/view?usp=sharing">[PDF]</a>
						</p>
						<p>SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks <a href="https://drive.google.com/file/d/1X5NSoS8mN9yLY0CelDLL__VjRXoVm2Wz/view?usp=sharing">[PDF]</a>
							<p>Dynamic Handover: Throw and Catch with Bimanual Hands <a href="https://drive.google.com/file/d/1_u3FiEyp-HdwKIdF4cLc2Gwf5bRkPUUk/view?usp=sharing">[PDF]</a>
						</p>
					</div>
				</div>
			</section>

			<section id="cfp" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Call for papers</h2>
						</header>

						<h3>Important dates (all times AoE)</h3>
						<ul>
							<li>Submissions open: April 19 2023</li>
							<li>Submission deadline: <del>June 2 2023 </del>&nbsp <b><ins>June 16 2023</ins></b> </li>
							<li>Decision notification: <del>June 16 2023</del>&nbsp <b><ins>June 26 2023</ins></b> </li>
							<li>Camera ready deadline: July 7 2023</li>
							<li>Workshop: July 14th 2023</li>
						</ul>

						<h3>Call for papers</h3>

						<p> Submission link: <a href="https://cmt3.research.microsoft.com/RSSLDM2023/">CMT</a>
						</p>

						<p>In this workshop, we aim to bring together machine learning and robotics researchers who
							work
							at the intersection of these fields.
							We invite researchers to submit work in the following or related areas (non-exhaustive
							list):</p>
						<ul>
							<li><strong>Data for Dexterous Manipulation:</strong></li>
							<ul>
								<li>Can human hand data for dexterous manipulation be collected in a general way
									using
									any
									expert-grade equipment? What is the data gap between human and robot hands?</li>
								<li>How can we improve current data collection methods, such as teleoperation, to
									facilitate
									large-scale data collection?</li>
							</ul>
							<li><strong>Computer Vision:</strong></li>
							<ul>
								<li>How can occlusion between objects and robot hands during dexterous manipulation
									be
									addressed?</li>
								<li>How can policies generalize to the open world outside the lab environment,
									considering
									the relatively unpredictable changes in outdoor lighting and the vast amount of
									information that needs to be processed?</li>
							</ul>
							<li><strong>Tactile Information:</strong></li>
							<ul>
								<li>How can tactile information help robots better accomplish tasks and perceive
									their
									environment?</li>
								<li>What kind of tactile information is best suited for dexterous robot hands, and
									can
									it
									compensate for the shortcomings of visual perception?</li>
							</ul>
							<li><strong>Robot learning</strong></li>
							<ul>
								<li>Will we see a unified and generalized model for most daily dexterous
									manipulation
									tasks
									or a specialized model for each individual task?</li>
								<li>How can learning-based policies handle dynamic tasks that require high-frequency
									control
									and detailed dynamics models?</li>
							</ul>
							<li>Any other related topics we might have forgotten in the list above &#128516;</li>
						</ul>


						<h4>Accepted Talks and Posters</h4>
						<p>Accepted papers will be presented in the form of posters (with lightning talks) or
							spotlight
							talks at the workshop. We encourage submissions of work in progress, as well as work
							that is
							not yet published. </p>

						<h3>Submission instructions</h3>

						<ul>
							<li>Kindly utilize the <a
									href="https://roboticsconference.org/docs/paper-template-latex.tar.gz">RSS 2023
									template</a> when submitting your paper. Including supplementary material is
								optional and only required if
								you wish to offer additional details or video demonstrations.
							</li>
							<li>Submissions should be short papers up to <b>4 pages in PDF format</b> (not counting
								references
								and an optional appendix, which can go over the limit)
							</li>
							<li>All submitted materials will undergo a double-blind review process. While this workshop
								will not produce
								formal official proceedings, the accepted papers will be accessible on the workshop
								website. As this does
								not count as an archival publication. Consequently,
								authors are at liberty to publish their work in archival journals or conferences.
							</li>
						</ul>

					</div>
				</div>
			</section>


			<section id="organizers" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Organizers</h2>
						</header>

						<ul class="features">
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="https://contactrika.github.io/img/rika_photo_one.jpg" alt="" />
								<h3><a href="https://contactrika.github.io/">Rika Antonova</a></br>Postdoctoral
									Scholar
									at
									Stanford</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/ah.jpg" alt="" />
								<h3><a href="https://ankurhanda.github.io/">Ankur Handa</a></br>Research Scientist
									at
									NVIDIA
								</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="images/bh.jpg" alt="" />
								<h3><a href="https://binghao-huang.github.io/">Binghao Huang</a></br>Ph.D. student
									at
									UIUC
								</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="https://www.lerrelpinto.com/authors/admin/avatar_hu4fd532323075888c71e9261145e3a1d5_40812_200x200_fill_q75_lanczos_center.jpg"
									alt="" />
								<h3><a href="https://lerrelpinto.com/">Lerrel
										Pinto</a></br>Assistant Professor at NYU</h3>
							</li>
							<!-- </ul>
						<ul class="features"> -->
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="https://yzqin.github.io/file/qyz_circle.png" alt="" />
								<h3><a href="https://yzqin.github.io/">Yuzhe Qin</a></br>Ph.D. student at UCSD</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="https://mahis.life/assets/images/self/mahi_profile_comp.jpeg" alt="" />
								<h3><a href="https://mahis.life/">Mahi Shafiullah</a></br>Ph.D. student at NYU
								</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="https://xiaolonw.github.io/static/profile.jpg" alt="" />
								<h3><a href="https://xiaolonw.github.io/">Xiaolong Wang</a></br>Assistant Professor
									at UCSD</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="https://ericyi.github.io/Li_Yi_files/ericyi.jpg" alt="" />
								<h3><a href="https://ericyi.github.io/">Li Yi</a></br>Assistant Professor at THU
								</h3>
							</li>
							<li>
								<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"
									src="http://robotouch.ri.cmu.edu/yuanwz/index_files/image002.jpg" alt="" />
								<h3><a href="http://robotouch.ri.cmu.edu/yuanwz/">Wenzhen Yuan</a></br>Assistant
									Professor
									at CMU</h3>
							</li>
						</ul>
					</div>
				</div>
			</section>

			<section id="contact" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Contact</h2>
						</header>
						<p>For questions and comments, please <a
								href="mailto:learning-dexterous-manipulation@googlegroups.com">contact us</a>.
						</p>
					</div>
				</div>
			</section>

		</div>

		<!-- Footer -->
		<footer id="footer">
			<p class="copyright">Copyright &copy; Mahi Shafiullah. Design: <a href="https://html5up.net">HTML5
					UP</a>.</br>Design inspired by <a
					href="http://bayesiandeeplearning.org/">http://bayesiandeeplearning.org/</a> by Yarin Gal and <a
					href="https://microsoft.github.io/robotics.pretraining.workshop.icra/">Pretraining for
					Robotics</a>
				by Sai Vemprala.</p>
		</footer>

	</div>

	<!-- Scripts  -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/skel.min.js"></script>
	<script src="assets/js/util.js"></script>
	<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="assets/js/main.js"></script>

	<!-- End of Statcounter Code -->
	<script src="assets/js/lightbox.js"></script>
</body>

</html>
